{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessing.preprocessor\n",
    "import spacy\n",
    "import nltk\n",
    "import importlib\n",
    "importlib.reload(preprocessing.preprocessor)\n",
    "from preprocessing.preprocessor import Preprocessor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_train_labels = pd.read_json('../../data/hackathon/train_labels_final.json', typ='series')\n",
    "series_train_set_ocr = pd.read_json('../../data/hackathon/train_set_ocr.json', typ='series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_text_json = series_train_set_ocr.reset_index()\n",
    "df_train_text_json.rename(columns={'index': 'file', 0: 'text'}, inplace=True)\n",
    "df_train_labels_json = series_train_labels.reset_index()\n",
    "df_train_labels_json.rename(columns={'index': 'file', 0: 'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_test_set_ocr = pd.read_json('../../data/hackathon/test_ocr_clean.json', typ='series')\n",
    "df_test = series_test_set_ocr.reset_index()\n",
    "df_test.rename(columns={'index': 'file', 0: 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ocr = pd.read_csv('../../data/ocr/ocr_hackaton.csv', usecols = (2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train_text_json, df_train_labels_json, on='file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(df_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.preprocess_text('text', lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = preprocessor.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_test = Preprocessor(df_test[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocessor_test\u001b[39m.\u001b[39mpreprocess_text(\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessor_test' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessor_test.preprocess_text('text', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('preprocessed_train_sample.csv', usecols=(1, 2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feature_engineering.feature_extractor\n",
    "import importlib\n",
    "importlib.reload(feature_engineering.feature_extractor)\n",
    "from feature_engineering.feature_extractor import FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureExtractor(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_terms = ['from', 'sent', 'sincerely', 'regards', 'subject',  'dear', 'memorandum', 'pit37', 'wniosek', 'udzielenie', 'pozwolenia', \n",
    "                 'budowlany', 'questionnaire', 'resume', 'curriculum',  'vitae', 'education', 'biographical',  'sketch', 'bio', 'oświadczenie',\n",
    "                 'odleglosc', 'umowa', 'dzieło', 'dzielo', 'samochód', 'pojazd', 'model', 'odstepuje', 'specification', \n",
    "                 'zabudowy', 'zeznanie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting feature for term  from\n",
      "extracting feature for term  sent\n",
      "extracting feature for term  sincerely\n",
      "extracting feature for term  regards\n",
      "extracting feature for term  subject\n",
      "extracting feature for term  dear\n",
      "extracting feature for term  memorandum\n",
      "extracting feature for term  pit37\n",
      "extracting feature for term  wniosek\n",
      "extracting feature for term  udzielenie\n",
      "extracting feature for term  pozwolenia\n"
     ]
    }
   ],
   "source": [
    "fe.feature_engineering('text', feature_terms, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "sklearn needs to be installed in order to use this module",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m----> 2\u001b[0m xgb \u001b[39m=\u001b[39m XGBClassifier()\n",
      "File \u001b[1;32md:\\Wiktor\\Studia\\Semestr 6\\hackING\\HackING2023\\hacking\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Wiktor\\Studia\\Semestr 6\\hackING\\HackING2023\\hacking\\lib\\site-packages\\xgboost\\sklearn.py:1396\u001b[0m, in \u001b[0;36mXGBClassifier.__init__\u001b[1;34m(self, objective, use_label_encoder, **kwargs)\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[39mif\u001b[39;00m use_label_encoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1395\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`use_label_encoder` is deprecated in 1.7.0.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1396\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(objective\u001b[39m=\u001b[39mobjective, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Wiktor\\Studia\\Semestr 6\\hackING\\HackING2023\\hacking\\lib\\site-packages\\xgboost\\sklearn.py:584\u001b[0m, in \u001b[0;36mXGBModel.__init__\u001b[1;34m(self, max_depth, max_leaves, max_bin, grow_policy, learning_rate, n_estimators, verbosity, objective, booster, tree_method, n_jobs, gamma, min_child_weight, max_delta_step, subsample, sampling_method, colsample_bytree, colsample_bylevel, colsample_bynode, reg_alpha, reg_lambda, scale_pos_weight, base_score, random_state, missing, num_parallel_tree, monotone_constraints, interaction_constraints, importance_type, gpu_id, validate_parameters, predictor, enable_categorical, feature_types, max_cat_to_onehot, max_cat_threshold, eval_metric, early_stopping_rounds, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    541\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    542\u001b[0m     max_depth: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    582\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    583\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SKLEARN_INSTALLED:\n\u001b[1;32m--> 584\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    585\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39msklearn needs to be installed in order to use this module\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m         )\n\u001b[0;32m    587\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators \u001b[39m=\u001b[39m n_estimators\n\u001b[0;32m    588\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m objective\n",
      "\u001b[1;31mImportError\u001b[0m: sklearn needs to be installed in order to use this module"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = fe.df.drop(['label', 'text', 'language'], axis = 1)\n",
    "y = fe.df['label']\n",
    "X_train, y_train, X_val, y_val = train_test_split(X, y, test_size=0.2, random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hacking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
